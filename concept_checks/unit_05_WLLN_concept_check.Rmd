---
title: "The Weak Law of Large Numbers"
output: 
  bookdown::gitbook
---
```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(magrittr)

library(ggplot2)
library(patchwork)
theme_set(theme_minimal())
```

# Weak Law of Large Numbers 
## Set Up the Space

Suppose that you have a coin that you are tossing that is unfair. But only a *little* bit unfair: the probability the coin comes up heads is 60%, while the probability that it comes up tails is 40%. 

Does this mean that in any given flip you will get a 0.6 and 0.4? No! Of course not, it is a probability statement. 

Although, for an important digression in your studies, you could think about election forecasts: in 2016 most models predicted that Hillary Clinton would defeat Donald Trump in the electoral college. A leading forecasting sight, fivethirtyeight.com put gave Trump a 29% change of winning. A lot of people casually interpreted this as though the outcome was going to be 29% Trump and 71% Clinton. Then, when the coin came up "Trump" there was a *lot* of conversation about how to interpret these model predictions. 

Returning to the task at hand: Does a P(heads = 0.60) mean that in 10 tosses you will get 6 that are heads and 4 that are tails? Still no! In expectation you will generate 60% of the tosses coming up heads, but in any single 10-toss sequences, there is randomness in this process that might lead any number of heads to be shown. 

So then, what do we mean when we say that there is a 60% chance that it will come up heads? And, how could we come to know this? 

Let's define a function that is called `toss_coin` that represents a coin that actually has an expected value of coming up heads that is 0.6. (This is little more than writing a named function that sets values for the already existing `rbinom` function. In general, this is bad coding practice to overload an already existing function, but it is useful for *teaching* this particular concept.) 

```{r}
toss_coin <- function(times) { 
  rbinom(n = times, size = 1, prob = 0.6)
  }
```

Then, we can sample from this coin's distribution by tossing it a small number of times.

```{r}
toss_coin(1)
```

Or, we can sample from this coin's distribution by tossing it a large number of times. 

```{r}
toss_coin(100)
```

## Weak Law of Large Numbers 

The Weak Law of Large Numbers (WLLN) says that as we take more samples, the sample average, $\overline{X}$, will converge in the probability limit to the expected value of $X$, $E[X]$. 

To use the formal definition: 

> Define $\overline{X}$ to be $\frac{1}{n} \sum_{i=1}^{n} X_{i}$. 
> 
> Then, if $X_1, \dots, X_n$ are i.i.d. random variables with finite, but positive variance $0 < V[X] < \infty$, 
> 
$$
\overline{X}_{(n)} \overset{p}\rightarrow E[X]
$$ 

This means that, even though we never *really* get to know the population parameter that is the $E[X]$, if we take a lot of draws and take the average of those draws, it become increasingly close. 

This is *very* useful! 

It means that we can *know* something that is *fundimentally* unknowable if we have enough data.  

Let's show this. 

1. First, make a dataframe that has 1,000 rows. On each of those rows, toss a single coin. 
2. Then, create a new varaible called `cumulative_mean` that is the running average of the data series from the first row to the last row. This is sort of a short-cut so that we don't have to run a **lot** of coin tosses; and, in some ways this breaks the rules for the data generating process (since it is actually only *one* data generating process that we're sub-sampling from) but it gets the point across. 
3. Third, and finally, plot the cumulative mean on the y-axis and the number of rows that are being considered on the x-axis. (Notice that you are plotting the same data, in two ways, once with a non-transformed x-axis, the other with a transformed x-axis)

```{r}
d <- data.frame(
  id = 1:1e4,
  x = toss_coin(1e4)
)

d <- d %>%  
  mutate(
    cumulative_mean = cummean(x)
  )

plot_not_logged <- d %>%
  ggplot(aes(x = id, y = cumulative_mean)) + 
  geom_line() + 
  labs(
    x = 'Number of Tosses (not logged scale)', 
    y = 'Running Average'
  )

plot_logged <- d %>%
  ggplot(aes(x = id, y = cumulative_mean)) + 
  scale_x_continuous(trans = 'log2') + 
  geom_line() + 
  labs(
    x = 'Number of Tosses (logged scale)', 
    y = 'Running Average'
  )

plot_not_logged / plot_logged
```

The WLLN is about a probability limit, and so as we add more data we'll get ever closer. 

But we also know a specific bound for finite $n$.  This is what we call *Chebyshev's Inequality for the Sample Mean*. 

Chebyshev's gives us a way to reason about the probably that the sample average will be more than some **particular** $\epsilon$ away from the true expectation. 

Specifically, suppose that we want to know:

> For the particular coin that we're tossing with a probability of landing heads = 60%, what is the probability that the sample mean is more than 0.01 away from the true expected value, given some number of tosses we have conducted, $n$?

Using Chebyshev's Inequality for the sample mean would proceed as follows: 

1. First, use Theorem 3.2.5 in FOAS to write out the left-hand side of statement of Chebyshev's (note that the proof is provided right after the concept is introduced in the textbook). . 

$$
\begin{align*}
  P \big[|\bar{X} - E[X]| \geq \epsilon \big] &=\\
  P \big[|\bar{X} - E[X]| \geq 0.01 \big] &= 
\end{align*}  
$$

The answer is that this probability will be less than 

$$
\begin{align*}
  &= \frac{V[X]}{\epsilon^{2}n}
\end{align*}
$$

We can know the value for $V[X]$ of our coin: 

$$
\begin{align*}
V[X] &= E[X^2] - E[X]^2 \\ 
     &= E[X^2] - (0.6)^2 \\ 
     &= E[X^2] - 0.36 \\ 
     &= \sum_{\forall x} x^2 \cdot f_x - 0.36 \\ 
     &= (0^2 \cdot 0.4 + 1^2 \cdot 0.6) - 0.36 \\ 
     &= (0 + 0.6) - 0.36 \\ 
V[X] &= 0.24
\end{align*}
$$

Or, since we've proven this in other work, if $X$ is a bernoulli rv, the $V[X] = p(1-p)$, 

$$
\begin{align*} 
  V[X] &= p(1-p) \\ 
       &= 0.6(1-0.6) \\ 
       &= 0.6 \cdot 0.4 \\ 
       &= 0.24
\end{align*} 
$$ 

And we have set $\epsilon$ to a specific value, $0.1$, so, $\epsilon^{2} = 0.01$. 

```{r}
d <- d %>%  
  mutate(
    chebyshev_prob = 0.24 / (0.01 * id),
    epsilon = sqrt(2.4 / id)
  )
```

### Basic, Exploratory Plot

Lets start to plot this with the key information that you want to communicate to yourself, how $\bar{X}$ changes as the number of samples increases. 

```{r}
d %>%  
  ggplot(aes(x = id, y = cumulative_mean)) + 
  scale_x_continuous(trans = 'log2') + 
  geom_line()
```

### Expository Plot

If you want to communicate this to others, you will have to take the time to place all of the context that you have in mind, into the plot. In this plot:

- We have placed the Chebyshev bounds in the plot 
- Kept the `cumulative_mean` line

But, notice that we have pulled the `aes(x = id, y = cumulative_mean)` out from the first `ggplot` call. This is because we do not want each of the plot elements to inherit this -- instead, we want the `geom_line` to take this, but the `geom_ribbon` to take a different set of y values. 

```{r}
cumulative_mean_not_logged <- d %>%  
  ggplot() + 
  geom_ribbon(
    aes(
      x = id, 
      ymin = .6 - epsilon, 
      ymax = .6 + epsilon), 
    fill = 'steelblue', alpha = 0.8) + 
  geom_line(aes(x = id, y = cumulative_mean), color = 'darkorange') + 
  geom_hline(yintercept = 0.6, color = 'steelblue') + 
  coord_cartesian(ylim = c(0,1)) + 
  labs(
    title = 'Sample Average Converges in Probability to E[X]',
    x = 'Number of Samples', 
    y = expression('Sample Average')) + 
  theme_minimal()

cumulative_mean_log_transformed <- cumulative_mean_not_logged + 
  scale_x_continuous(trans = 'log2')
  
cumulative_mean_not_logged | cumulative_mean_log_transformed  
```

## Theoretical Implications 

Return back up a level from the coding to the thinking. 

> What are we guaranteed by the WLLN? 

So long as a random variable has finite variance, then as the number of samples from that random variable increases, the average of those samples will converge to the expected value of that random variable. 

This is really powerful! The only condition that we have placed on this statement is that the random variable have finite variance -- nothing else. 

Through Chebyshev's Inequality for the sample mean, we can characterize how quickly the sample averages will converge toward the expected value, at the rate of $\sqrt{\frac{V[X]}{n}}$. 